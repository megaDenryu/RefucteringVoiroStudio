とりあえずあれをしたいな
えーとあれですよあれ、あれなんです。思い出した。
確かね、なんかどんなんだったかというと、コメント読み上げを前まで実装してたのにさ、なんか機能しなくなってるんですよね。
多分あれですよ、複数キャラ出せるようにしたじゃないですか？あれを実装したときに、キャラクターの認識がバグったんですよ。
どういうことかというとね、まずどこから説明すればいいかな。
順番がわからない。
あれだ。
まず今まではどうやって動いてたかを考えよう。
コメントが来るじゃないですか？そしたら次は何が起こると思う？
コメントを誰が送ったかが大事なんですよ。
そう、コメントを送った人をまず確認して、その人がどのキャラクターを選択してたかの辞書を見るんですよ。
読み上げキャラをそこで、辞書に載ってたキャラに設定するの。
もし辞書に登録されてなかったら読み上げキャラをデフォルトのキャラに設定するよ。
さらにもし辞書に登録されてても今そのキャラを出してなかったら、読み上げキャラをデフォルトのキャラに設定するよ。

さてバグの原因だけど、問題は複数同じキャラを出してるとき辞書はどうなってるんだ？みたいになるよね。そこがバグってるんだ。
間違いないよ。でも私今そこがどうなってるか記憶にないから確認しなきゃ。

えーと
いまどうなってるんだ

##
こういう時にAIが勝手に調べてくれると嬉しいよね
そして任意の文章単位に対して四角で囲ってそこから線が伸びてAIの思考が記述されたノードが表示される感じ。
実際これが人間がいろいろなものを見たときの感覚に近い。
これなら文章を切り取る部分と、切り取った文章に対して言及する部分に分けてＬＬＭプロセスを構築でき、そんなに複雑にはならないと思う。

文章を切り取るというのが地味に大変だと思うけどね。
文章を切り取る理由はＡＩの回答がどの部分についての回答なのかわからなくなるから。
そして今のスレッド式への不満として、過去のここの部分について質問がしたい。みたいな用途があるから。
つまり、AIが自分でどこかを切り取る必要はなくて、人間が指定できるようにすればいい。
ただし重要なのスレッドでの未来の情報も含む全ての情報を踏まえて過去について言及するということが必要になる。

やはりAIにやってもらいたいのは過去への言及なのか。
しかしこれを口頭でやるのはやはり厳しいものがある。
人間が過去を思い出し、適切な言葉で指定しなければならないから。そしてＡＩが話しかけられているかを適切に判断しないといけないから。

適切に判断するのは難しいので３つのパーツから成る判断モデルが思い浮かんだ。
1. 好奇心モデル：自分の名前が含まれてるかどうかで判断して会話をスタートする。もっというＡＩ自身が興味がある話題かどうかで判断する。あらかじめ好きなものは登録しておく。
2. 会話をどこで切り上げるかの減衰モデル。１ターン前に会話していれば次のターンいきなり会話欲が０になることはない。しかし、そのまま維持されることもない。どちらかというと減衰していく。増加していくこともある。欲望増減モデル。
3. 

    流れ
    1. ユーザーが文章を入力
    2. AIの耳に入る
    3. 好奇心モデルが興味があるか判定する。なければreturn
    4. 

## Grokとの会話
今AIと音声とマイクで会話するゲームを作っているんだけど、ユーザー音声→音声認識→文字情報→LLM→返答→TTS→ＡＩ音声のようなフローで作ってるけど、実際には
文字情報に対して直接返答しないで欲しくて、返答するべきある程度のまとまりになってるかどうかを判断してＬＬＭに文章生成させたいんだけど、それの判別が上手くかない。
少ない単位のまますぐに返信してしまいます。
他のUIで返信するタイミングを指示するみたいなのはリアリティにかけるからいやです。ChatGPTにはそういう会話モードがあるが、あれはどのように実装しているのか

無音検出・タイムアウト・バッファリングと閾値設定はすでに試していますが、それは定数的過ぎて結局うまくいきませんでした。
それでさらにそこから文の完全性やコンテキストベースの待機が必要そうだという考えに私も至り、試しましたが、当時はJson出力の技術などが未発達で実装に苦労し、複雑な判定をできませんでした。これにはまだ可能性があると思います。
そしてさらに進んで、AI側に欲求パラメータと所有エネルギーパラメータを入れ、返答欲求が閾値を超えたときに返答するが、エネルギーに応じて返答欲求にデバフがかかるという仕組みなども考えていますがまだ実装していません。

現在はBaseModelによる構造化出力技術が発達してので可能になってきてると思うのでこれらについてより深く考察してほしいです。
できれば具体的な実装も欲しいです。

とてもよさそうな初期案です。
私にはすでにオブジェクト指向ベースのある程度の実装資産があり、そこに上手く統合できるようにさらに洗練したいです。

今考えている統合の仕方としては以下のようなinterfaceをabstractクラスとして考えています。
``` python
class c未完全入力:
    pass

class c完全入力:
    pass

class I返答判定機(ABC):
    @abstractmethod
    def f判定(self, a未完全入力: c未完全入力) -> c未完全入力|c完全入力:
        pass
```

このI返答判定機としてLLMエージェントを実装してほしいです。

また、次のようなLLMエージェントにコンポジションして使うためのクラスを定義してあります。
``` python
class MessageQuery(TypedDict):
    role: Literal['system', 'user', 'assistant']
    content: str

class ChatGptApiUnit:
    """
    責務:APIにリクエストを送り、結果を受け取るだけ。クエリの調整は行わない。
    """

    def __init__(self,gpt_model:str,test_mode:bool = True):
        try:
            api_key = JsonAccessor.loadOpenAIAPIKey()
            self.client = OpenAI(api_key = api_key)
            self.async_client = AsyncOpenAI(api_key = api_key)
            self.test_mode = test_mode
            self.gpt_model = gpt_model

        except Exception as e:
            print("APIキーの読み込みに失敗しました。")
            raise e
    def generateResponseStructured(self,message_query:list[MessageQuery], model:Type[ResponseFormatT]) -> ResponseFormatT|Literal["テストモードです"]|None:
        if self.test_mode == True:
            print("テストモードです")
            return "テストモードです"
        completion = self.client.beta.chat.completions.parse(
                model=self.gpt_model,
                messages=message_query, # type: ignore
                response_format=model,
        )

        return completion.choices[0].message.parsed
    
    async def asyncGenerateResponseStructured(self,message_query:list[MessageQuery], model:Type[ResponseFormatT]) -> ResponseFormatT|Literal["テストモードです"]|None:
        if self.test_mode == True:
            print("テストモードです")
            return "テストモードです"
        completion = await self.async_client.beta.chat.completions.parse(
                model=self.gpt_model,
                messages=message_query, # type: ignore
                response_format=model,
        )

        return completion.choices[0].message.parsed

```
これを使用して書いてほしいです。